{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 23:48:23.352880: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-12-22 23:48:29.350823: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-12-22 23:48:29.439673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-12-22 23:48:29.440261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:81:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-12-22 23:48:29.440331: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-12-22 23:48:29.449946: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-12-22 23:48:29.450064: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-12-22 23:48:29.454287: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-12-22 23:48:29.455891: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-12-22 23:48:29.457193: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-12-22 23:48:29.473773: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-12-22 23:48:29.474153: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-12-22 23:48:29.475763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2022-12-22 23:48:29.476483: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-22 23:48:29.948047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-12-22 23:48:29.948319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:81:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-12-22 23:48:29.949197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2022-12-22 23:48:29.949278: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-12-22 23:48:30.783355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-12-22 23:48:30.783397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2022-12-22 23:48:30.783405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N \n",
      "2022-12-22 23:48:30.783409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N \n",
      "2022-12-22 23:48:30.784534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\n",
      "2022-12-22 23:48:30.785148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10160 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1)\n",
      "2022-12-22 23:48:30.790517: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 8.56M (8978432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-12-22 23:48:30.791386: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 7.71M (8080640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-12-22 23:48:30.792247: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 6.94M (7272704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-12-22 23:48:30.793099: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 6.24M (6545664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from lib.methods import *\n",
    "from lib.models import *\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading SIPI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIPI dataset was preprocessed using prepare_dataset notebook, so these three folders already contain Detritus/Non-Detritus images.\n",
    "\n",
    "Training: 70%\n",
    "Validation: 15%\n",
    "Testing: 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetName = '../../Detritus/Dataset-Detritus-Bubble'\n",
    "\n",
    "train_dir = DatasetName+'/train'\n",
    "validation_dir =  DatasetName+'/val'\n",
    "test_dir = DatasetName+'/test'\n",
    "\n",
    "test_all_class_dir = '../../Detritus/Dataset-Detritus-Bubble/test'\n",
    "train_all_class_dir = 'Dataset-Detritus-Bubble/train'\n",
    "val_all_class_dir = 'Dataset-Detritus-Bubble/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three datasets are loaded using keras preprocessing method *image_dataset_from_directory*. Both the batch size and the image size hyperparameters where tested using different values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38391 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(train_dir,\n",
    "shuffle=False,\n",
    "batch_size=BATCH_SIZE,\n",
    "image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8226 files belonging to 2 classes.\n",
      "Found 8227 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = image_dataset_from_directory(validation_dir,\n",
    "shuffle=False,\n",
    "batch_size=BATCH_SIZE,\n",
    "image_size=IMG_SIZE)\n",
    "\n",
    "test_dataset = image_dataset_from_directory(test_dir,\n",
    "shuffle=False,\n",
    "batch_size=BATCH_SIZE,\n",
    "image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8227 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = ImageDataGenerator()\n",
    "test_data_generator = test_generator.flow_from_directory(test_all_class_dir,\n",
    "shuffle=False,\n",
    "batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamers to be used in all models\n",
    "base_learning_rate = 0.001\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "TRAINING_EPOCHS = 200\n",
    "\n",
    "\n",
    "# Callback Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                patience=20,\n",
    "                                                mode=\"min\",\n",
    "                                               restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model, epochs:  200\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 23:48:33.076986: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-12-22 23:48:33.097062: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2100035000 Hz\n",
      "2022-12-22 23:48:43.947693: W tensorflow/core/common_runtime/bfc_allocator.cc:456] Allocator (GPU_0_bfc) ran out of memory trying to allocate 9.38MiB (rounded to 9830400)requested by op SameWorkerRecvDone\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-12-22 23:48:43.947763: I tensorflow/core/common_runtime/bfc_allocator.cc:991] BFCAllocator dump for GPU_0_bfc\n",
      "2022-12-22 23:48:43.947788: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (256): \tTotal Chunks: 68, Chunks in use: 66. 17.0KiB allocated for chunks. 16.5KiB in use in bin. 3.5KiB client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947803: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947818: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (1024): \tTotal Chunks: 4, Chunks in use: 4. 6.5KiB allocated for chunks. 6.5KiB in use in bin. 6.1KiB client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947833: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (2048): \tTotal Chunks: 6, Chunks in use: 6. 12.0KiB allocated for chunks. 12.0KiB in use in bin. 12.0KiB client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947846: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947859: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947874: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (16384): \tTotal Chunks: 3, Chunks in use: 3. 61.8KiB allocated for chunks. 61.8KiB in use in bin. 54.0KiB client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947888: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (32768): \tTotal Chunks: 1, Chunks in use: 0. 36.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947904: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (65536): \tTotal Chunks: 3, Chunks in use: 3. 216.0KiB allocated for chunks. 216.0KiB in use in bin. 216.0KiB client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947920: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (131072): \tTotal Chunks: 6, Chunks in use: 6. 864.0KiB allocated for chunks. 864.0KiB in use in bin. 864.0KiB client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947932: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947945: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947959: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 3. 4.43MiB allocated for chunks. 4.43MiB in use in bin. 3.38MiB client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947971: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947983: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.947996: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.948008: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.948021: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.948033: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.948048: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.948061: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-12-22 23:48:43.948075: I tensorflow/core/common_runtime/bfc_allocator.cc:1014] Bin for 9.38MiB was 8.00MiB, Chunk State: \n",
      "2022-12-22 23:48:43.948085: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Next region of size 5891328\n",
      "2022-12-22 23:48:43.948104: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800000 of size 1280 by op ScratchBuffer action_count 6587962511847794200 step 0 next 1\n",
      "2022-12-22 23:48:43.948116: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800500 of size 256 by op AssignVariableOp action_count 6587962511847794201 step 0 next 2\n",
      "2022-12-22 23:48:43.948127: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800600 of size 256 by op AssignVariableOp action_count 6587962511847794202 step 0 next 3\n",
      "2022-12-22 23:48:43.948137: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800700 of size 256 by op AssignVariableOp action_count 6587962511847794203 step 0 next 4\n",
      "2022-12-22 23:48:43.948149: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800800 of size 256 by op Fill action_count 6587962511847794213 step 0 next 8\n",
      "2022-12-22 23:48:43.948160: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800900 of size 256 by op Fill action_count 6587962511847794214 step 0 next 11\n",
      "2022-12-22 23:48:43.948171: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800a00 of size 256 by op Sub action_count 6587962511847794216 step 0 next 13\n",
      "2022-12-22 23:48:43.948181: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800b00 of size 256 by op Sub action_count 6587962511847794217 step 0 next 14\n",
      "2022-12-22 23:48:43.948191: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800c00 of size 256 by op Fill action_count 6587962511847794224 step 0 next 12\n",
      "2022-12-22 23:48:43.948202: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800d00 of size 256 by op Sub action_count 6587962511847794226 step 0 next 18\n",
      "2022-12-22 23:48:43.948212: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800e00 of size 256 by op Sub action_count 6587962511847794227 step 0 next 5\n",
      "2022-12-22 23:48:43.948222: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a800f00 of size 256 by op Sub action_count 6587962511847794205 step 0 next 6\n",
      "2022-12-22 23:48:43.948233: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a801000 of size 256 by op Sub action_count 6587962511847794206 step 0 next 7\n",
      "2022-12-22 23:48:43.948243: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a801100 of size 256 by op Fill action_count 6587962511847794234 step 0 next 17\n",
      "2022-12-22 23:48:43.948253: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a801200 of size 256 by op Sub action_count 6587962511847794236 step 0 next 21\n",
      "2022-12-22 23:48:43.948264: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a801300 of size 256 by op Sub action_count 6587962511847794237 step 0 next 22\n",
      "2022-12-22 23:48:43.948274: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a801400 of size 256 by op Fill action_count 6587962511847794244 step 0 next 25\n",
      "2022-12-22 23:48:43.948284: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a801500 of size 256 by op Fill action_count 6587962511847794252 step 0 next 27\n",
      "2022-12-22 23:48:43.948295: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a801600 of size 256 by op Sub action_count 6587962511847794254 step 0 next 29\n",
      "2022-12-22 23:48:43.948307: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a801700 of size 256 by op Sub action_count 6587962511847794255 step 0 next 30\n",
      "2022-12-22 23:48:43.948318: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a801800 of size 256 by op Sub action_count 6587962511847794264 step 0 next 9\n",
      "2022-12-22 23:48:43.948329: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a801900 of size 1792 by op Add action_count 6587962511847794210 step 0 next 10\n",
      "2022-12-22 23:48:43.948340: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a802000 of size 2048 by op Fill action_count 6587962511847794262 step 0 next 28\n",
      "2022-12-22 23:48:43.948350: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a802800 of size 256 by op Fill action_count 6587962511847794272 step 0 next 35\n",
      "2022-12-22 23:48:43.948360: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a802900 of size 256 by op Fill action_count 6587962511847794273 step 0 next 38\n",
      "2022-12-22 23:48:43.948371: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a802a00 of size 256 by op Fill action_count 6587962511847794274 step 0 next 39\n",
      "2022-12-22 23:48:43.948381: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a802b00 of size 256 by op AssignVariableOp action_count 6587962511847794275 step 0 next 40\n",
      "2022-12-22 23:48:43.948392: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a802c00 of size 256 by op Fill action_count 6587962511847794283 step 0 next 41\n",
      "2022-12-22 23:48:43.948402: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a802d00 of size 256 by op Equal action_count 6587962511847794277 step 0 next 42\n",
      "2022-12-22 23:48:43.948413: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a802e00 of size 256 by op AssignVariableOp action_count 6587962511847794284 step 0 next 43\n",
      "2022-12-22 23:48:43.948423: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a802f00 of size 256 by op AssignVariableOp action_count 6587962511847794285 step 0 next 33\n",
      "2022-12-22 23:48:43.948434: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803000 of size 256 by op Sub action_count 6587962511847794265 step 0 next 34\n",
      "2022-12-22 23:48:43.948444: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803100 of size 256 by op AssignVariableOp action_count 6587962511847794286 step 0 next 44\n",
      "2022-12-22 23:48:43.948454: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803200 of size 256 by op AssignVariableOp action_count 6587962511847794287 step 0 next 45\n",
      "2022-12-22 23:48:43.948465: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803300 of size 256 by op Fill action_count 6587962511847794288 step 0 next 46\n",
      "2022-12-22 23:48:43.948475: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803400 of size 256 by op Fill action_count 6587962511847794290 step 0 next 48\n",
      "2022-12-22 23:48:43.948486: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803500 of size 256 by op Fill action_count 6587962511847794292 step 0 next 49\n",
      "2022-12-22 23:48:43.948496: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803600 of size 256 by op Fill action_count 6587962511847794294 step 0 next 51\n",
      "2022-12-22 23:48:43.948506: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803700 of size 256 by op Fill action_count 6587962511847794296 step 0 next 52\n",
      "2022-12-22 23:48:43.948517: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803800 of size 256 by op Fill action_count 6587962511847794298 step 0 next 54\n",
      "2022-12-22 23:48:43.948527: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803900 of size 256 by op Fill action_count 6587962511847794302 step 0 next 36\n",
      "2022-12-22 23:48:43.948540: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a803a00 of size 2048 by op Add action_count 6587962511847794269 step 0 next 37\n",
      "2022-12-22 23:48:43.948550: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a804200 of size 1792 by op Fill action_count 6587962511847794289 step 0 next 47\n",
      "2022-12-22 23:48:43.948561: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a804900 of size 26368 by op Fill action_count 6587962511847794291 step 0 next 16\n",
      "2022-12-22 23:48:43.948572: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a80b000 of size 18432 by op Add action_count 6587962511847794221 step 0 next 15\n",
      "2022-12-22 23:48:43.948583: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a80f800 of size 73728 by op Fill action_count 6587962511847794293 step 0 next 50\n",
      "2022-12-22 23:48:43.948593: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a821800 of size 2048 by op Fill action_count 6587962511847794300 step 0 next 55\n",
      "2022-12-22 23:48:43.948604: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a822000 of size 2048 by op Fill action_count 6587962511847794301 step 0 next 56\n",
      "2022-12-22 23:48:43.948614: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a822800 of size 1792 by op Fill action_count 6587962511847794303 step 0 next 57\n",
      "2022-12-22 23:48:43.948625: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a822f00 of size 256 by op Fill action_count 6587962511847794304 step 0 next 58\n",
      "2022-12-22 23:48:43.948635: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a823000 of size 18432 by op Fill action_count 6587962511847794305 step 0 next 59\n",
      "2022-12-22 23:48:43.948646: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a827800 of size 256 by op Fill action_count 6587962511847794306 step 0 next 60\n",
      "2022-12-22 23:48:43.948656: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a827900 of size 256 by op Fill action_count 6587962511847794308 step 0 next 62\n",
      "2022-12-22 23:48:43.948686: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a827a00 of size 256 by op Fill action_count 6587962511847794310 step 0 next 64\n",
      "2022-12-22 23:48:43.948698: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a827b00 of size 256 by op Fill action_count 6587962511847794312 step 0 next 66\n",
      "2022-12-22 23:48:43.948709: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a827c00 of size 2048 by op Fill action_count 6587962511847794314 step 0 next 67\n",
      "2022-12-22 23:48:43.948719: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a828400 of size 2048 by op Fill action_count 6587962511847794315 step 0 next 68\n",
      "2022-12-22 23:48:43.948730: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a828c00 of size 256 by op Fill action_count 6587962511847794316 step 0 next 69\n",
      "2022-12-22 23:48:43.948740: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a828d00 of size 256 by op Fill action_count 6587962511847794317 step 0 next 70\n",
      "2022-12-22 23:48:43.948750: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a828e00 of size 256 by op Fill action_count 6587962511847794318 step 0 next 71\n",
      "2022-12-22 23:48:43.948761: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a828f00 of size 256 by op Adam/add/y action_count 6587962511847794319 step 0 next 72\n",
      "2022-12-22 23:48:43.948771: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829000 of size 256 by op Adam/Const action_count 6587962511847794320 step 0 next 73\n",
      "2022-12-22 23:48:43.948784: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829100 of size 256 by op ConstantFolding/model/sequential/random_rotation/rotation_matrix/truediv_recip action_count 6587962511847794321 step 0 next 74\n",
      "2022-12-22 23:48:43.948798: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829200 of size 256 by op model/sequential/random_rotation/stateful_uniform/min action_count 6587962511847794322 step 0 next 75\n",
      "2022-12-22 23:48:43.948809: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829300 of size 256 by op model/sequential/random_rotation/stateful_uniform/sub action_count 6587962511847794323 step 0 next 76\n",
      "2022-12-22 23:48:43.948820: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829400 of size 256 by op model/sequential/random_rotation/rotation_matrix/zeros/Const action_count 6587962511847794324 step 0 next 77\n",
      "2022-12-22 23:48:43.948831: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829500 of size 256 by op ConstantFolding/model/tf.math.truediv/truediv_recip action_count 6587962511847794325 step 0 next 78\n",
      "2022-12-22 23:48:43.948841: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829600 of size 256 by op Adam/gradients/ones action_count 6587962511847794326 step 0 next 79\n",
      "2022-12-22 23:48:43.948852: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829700 of size 256 by op Const_1 action_count 6587962511847794327 step 0 next 80\n",
      "2022-12-22 23:48:43.948863: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829800 of size 256 by op gradient_tape/binary_crossentropy/logistic_loss/add/x action_count 6587962511847794328 step 0 next 81\n",
      "2022-12-22 23:48:43.948874: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829900 of size 256 by op Adam/Adam/Const action_count 6587962511847794329 step 0 next 82\n",
      "2022-12-22 23:48:43.948884: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829a00 of size 256 by op SameWorkerRecvDone action_count 6587962511847794330 step 0 next 83\n",
      "2022-12-22 23:48:43.948895: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829b00 of size 256 by op SameWorkerRecvDone action_count 6587962511847794331 step 0 next 84\n",
      "2022-12-22 23:48:43.948905: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829c00 of size 256 by op SameWorkerRecvDone action_count 6587962511847794332 step 0 next 85\n",
      "2022-12-22 23:48:43.948916: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829d00 of size 256 by op Adam/Pow action_count 6587962511847794336 step 9407229044327652867 next 86\n",
      "2022-12-22 23:48:43.948927: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a829e00 of size 256 by op Adam/Cast_1 action_count 6587962511847794334 step 9407229044327652867 next 87\n",
      "2022-12-22 23:48:43.948938: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Free  at 7fcd8a829f00 of size 256 by op UNUSED action_count 6587962511847794344 step 0 next 88\n",
      "2022-12-22 23:48:43.948949: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a82a000 of size 256 by op model/sequential/random_flip/random_flip_left_right/random_uniform/RandomUniform action_count 6587962511847794338 step 9407229044327652867 next 89\n",
      "2022-12-22 23:48:43.948960: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a82a100 of size 256 by op model/sequential/random_flip/random_flip_left_right/sub action_count 6587962511847794339 step 9407229044327652867 next 90\n",
      "2022-12-22 23:48:43.948971: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Free  at 7fcd8a82a200 of size 256 by op UNUSED action_count 6587962511847794343 step 0 next 91\n",
      "2022-12-22 23:48:43.948982: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a82a300 of size 256 by op Cast action_count 6587962511847794341 step 9407229044327652867 next 92\n",
      "2022-12-22 23:48:43.948993: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a82a400 of size 256 by op binary_crossentropy/Cast action_count 6587962511847794342 step 9407229044327652867 next 93\n",
      "2022-12-22 23:48:43.949003: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Free  at 7fcd8a82a500 of size 37632 by op UNUSED action_count 0 step 0 next 20\n",
      "2022-12-22 23:48:43.949014: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a833800 of size 73728 by op Add action_count 6587962511847794231 step 0 next 19\n",
      "2022-12-22 23:48:43.949025: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a845800 of size 147456 by op Fill action_count 6587962511847794295 step 0 next 24\n",
      "2022-12-22 23:48:43.949036: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a869800 of size 147456 by op Add action_count 6587962511847794241 step 0 next 23\n",
      "2022-12-22 23:48:43.949046: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a88d800 of size 147456 by op Add action_count 6587962511847794249 step 0 next 26\n",
      "2022-12-22 23:48:43.949057: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a8b1800 of size 73728 by op Fill action_count 6587962511847794307 step 0 next 61\n",
      "2022-12-22 23:48:43.949067: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a8c3800 of size 147456 by op Fill action_count 6587962511847794309 step 0 next 63\n",
      "2022-12-22 23:48:43.949078: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a8e7800 of size 147456 by op Fill action_count 6587962511847794311 step 0 next 65\n",
      "2022-12-22 23:48:43.949088: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8a90b800 of size 1990656 by op Fill action_count 6587962511847794313 step 0 next 31\n",
      "2022-12-22 23:48:43.949100: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8aaf1800 of size 1179648 by op Add action_count 6587962511847794259 step 0 next 32\n",
      "2022-12-22 23:48:43.949110: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8ac11800 of size 147456 by op Fill action_count 6587962511847794297 step 0 next 53\n",
      "2022-12-22 23:48:43.949122: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at 7fcd8ac35800 of size 1477888 by op Fill action_count 6587962511847794299 step 0 next 18446744073709551615\n",
      "2022-12-22 23:48:43.949131: I tensorflow/core/common_runtime/bfc_allocator.cc:1051]      Summary of in-use Chunks by size: \n",
      "2022-12-22 23:48:43.949146: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 66 Chunks of size 256 totalling 16.5KiB\n",
      "2022-12-22 23:48:43.949161: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-12-22 23:48:43.949174: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 3 Chunks of size 1792 totalling 5.2KiB\n",
      "2022-12-22 23:48:43.949188: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 6 Chunks of size 2048 totalling 12.0KiB\n",
      "2022-12-22 23:48:43.949201: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 2 Chunks of size 18432 totalling 36.0KiB\n",
      "2022-12-22 23:48:43.949214: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 1 Chunks of size 26368 totalling 25.8KiB\n",
      "2022-12-22 23:48:43.949227: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 3 Chunks of size 73728 totalling 216.0KiB\n",
      "2022-12-22 23:48:43.949241: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 6 Chunks of size 147456 totalling 864.0KiB\n",
      "2022-12-22 23:48:43.949252: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 1 Chunks of size 1179648 totalling 1.12MiB\n",
      "2022-12-22 23:48:43.949264: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 1 Chunks of size 1477888 totalling 1.41MiB\n",
      "2022-12-22 23:48:43.949275: I tensorflow/core/common_runtime/bfc_allocator.cc:1054] 1 Chunks of size 1990656 totalling 1.90MiB\n",
      "2022-12-22 23:48:43.949287: I tensorflow/core/common_runtime/bfc_allocator.cc:1058] Sum Total of in-use chunks: 5.58MiB\n",
      "2022-12-22 23:48:43.949299: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] total_region_allocated_bytes_: 5891328 memory_limit_: 8978432 available bytes: 3087104 curr_region_allocation_bytes_: 17956864\n",
      "2022-12-22 23:48:43.949321: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Stats: \n",
      "Limit:                         8978432\n",
      "InUse:                         5853184\n",
      "MaxInUse:                      5853696\n",
      "NumAllocs:                         118\n",
      "MaxAllocSize:                  1990656\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-12-22 23:48:43.949347: W tensorflow/core/common_runtime/bfc_allocator.cc:467] ***************************************xxxxxxxxxxxxx*******************************************xxxxx\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;c505833ae26c9023;/job:localhost/replica:0/task:0/device:GPU:0;edge_25_IteratorGetNext;0:0\n\t [[{{node IteratorGetNext/_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1834]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30565/2808904025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINING_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mshow_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/detritus-recognition/lib/methods.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, train_dataset, validation_dataset, lr, callbacks)\u001b[0m\n\u001b[1;32m     22\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     history = model.fit(train_dataset,\n\u001b[0m\u001b[1;32m     25\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python37/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:CPU:0;c505833ae26c9023;/job:localhost/replica:0/task:0/device:GPU:0;edge_25_IteratorGetNext;0:0\n\t [[{{node IteratorGetNext/_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1834]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.model_a', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks = [mcp_save, early_stopping]\n",
    "\n",
    "model_A = get_model_A(0.0, True, 0.0)\n",
    "history = train_model(model_A, TRAINING_EPOCHS, train_dataset, validation_dataset, base_learning_rate, callbacks)\n",
    "show_plot(history, 0.0)\n",
    "test_accuracy(model_A, test_dataset)\n",
    "print_tsne(model_A, test_dataset, 2500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.model_b', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks = [mcp_save, early_stopping]\n",
    "\n",
    "model_B = get_model_B(0.0, True, 0.0)\n",
    "history = train_model(model_B, TRAINING_EPOCHS, train_dataset, validation_dataset, base_learning_rate, callbacks)\n",
    "show_plot(history, 0.0)\n",
    "test_accuracy(model_B, test_dataset)\n",
    "print_tsne(model_B, test_dataset, 2500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mobile_net(drop_value=0.0, data_aug=False, l2_reg=0.0):\n",
    "    inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "    \n",
    "    if data_aug:\n",
    "        dag = data_augmentation(inputs)\n",
    "        processed_input =  tf.keras.applications.mobilenet_v2.preprocess_input(dag)\n",
    "    else:\n",
    "        processed_input =  tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "    \n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=None)\n",
    "    \n",
    "    x = base_model(processed_input)\n",
    "    \n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    \n",
    "    x = global_average_layer(x)\n",
    "    \n",
    "    if drop_value > 0:\n",
    "        x = keras.layers.Dropout(drop_value)(x)\n",
    "    \n",
    "    if l2_reg > 0:\n",
    "        x = keras.layers.Dense(1, kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                bias_regularizer=regularizers.l2(l2_reg),\n",
    "                activity_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    else:\n",
    "        x = keras.layers.Dense(1)(x)\n",
    "    return keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNetV2 Model From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.mobile_net_scratch', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks = [mcp_save, early_stopping]\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "mobileNetModelSc = get_mobile_net(0.0, True, 0.0)\n",
    "history = train_model(mobileNetModelSc, TRAINING_EPOCHS, train_dataset, validation_dataset, base_learning_rate, callbacks)\n",
    "show_plot(history, 0.0)\n",
    "test_accuracy(mobileNetModelSc, test_dataset)\n",
    "print_tsne(mobileNetModelSc, test_dataset, 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(acc, val_acc, loss, val_loss):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,1.0])\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.dense_net_ft', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks = [mcp_save, early_stopping]\n",
    "\n",
    "def do_transfer_learning(drop_value=0.0, data_aug=False, l2_reg=0.0, input_model=None, base_model=None):\n",
    "    dense_model = input_model\n",
    "    dense_base_model = base_model\n",
    "    if (input_model is None):\n",
    "        dense_base_model = tf.keras.applications.DenseNet201(input_shape=IMG_SHAPE,\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "\n",
    "        dense_base_model.trainable = False\n",
    "\n",
    "        dense_preprocess_input = tf.keras.applications.densenet.preprocess_input\n",
    "        dense_global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        dense_prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "        dense_inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "        if data_aug:\n",
    "            dag = data_augmentation(dense_inputs)\n",
    "            dense_x =  tf.keras.applications.densenet.preprocess_input(dag)\n",
    "        else:\n",
    "            dense_x =  tf.keras.applications.densenet.preprocess_input(dense_inputs)\n",
    "        dense_x = dense_base_model(dense_x, training=False)\n",
    "        dense_x = dense_global_average_layer(dense_x)\n",
    "        if drop_value > 0:\n",
    "            dense_x = keras.layers.Dropout(drop_value)(dense_x)\n",
    "        dense_outputs = dense_prediction_layer(dense_x)\n",
    "        dense_model = tf.keras.Model(dense_inputs, dense_outputs)\n",
    "\n",
    "    dense_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    dense_history = dense_model.fit(train_dataset,\n",
    "                    epochs=TRAINING_EPOCHS,\n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=callbacks)\n",
    "    \n",
    "    acc = dense_history.history['accuracy']\n",
    "    val_acc = dense_history.history['val_accuracy']\n",
    "\n",
    "    loss = dense_history.history['loss']\n",
    "    val_loss = dense_history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title(\"Training and Validation Accuracy with Dropout {0}\".format(drop_value))\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,1.0])\n",
    "    plt.title(\"Training and Validation Loss with Dropout {0}\".format(drop_value))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    dense_base_model.trainable = True\n",
    "    \n",
    "    dense_fine_tune_at = 660\n",
    "    \n",
    "    for layer in dense_base_model.layers[:dense_fine_tune_at]:\n",
    "        layer.trainable =  False\n",
    "        \n",
    "    dense_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    total_epochs =  TRAINING_EPOCHS + TRAINING_EPOCHS\n",
    "\n",
    "    dense_history_fine = dense_model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=dense_history.epoch[-1],\n",
    "                         validation_data=validation_dataset,\n",
    "                         callbacks=callbacks)\n",
    "    \n",
    "    acc += dense_history_fine.history['accuracy']\n",
    "    val_acc += dense_history_fine.history['val_accuracy']\n",
    "\n",
    "    loss += dense_history_fine.history['loss']\n",
    "    val_loss += dense_history_fine.history['val_loss']\n",
    "    \n",
    "    show_plot(acc, val_acc, loss, val_loss)\n",
    "    \n",
    "    loss, accuracy = dense_model.evaluate(test_dataset)\n",
    "    print('Test accuracy :', accuracy)\n",
    "    \n",
    "    \n",
    "    print_tsne(dense_model, test_dataset, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_net(drop_value=0.0, data_aug=False, l2_reg=0.0):\n",
    "    dense_base_model = tf.keras.applications.DenseNet201(input_shape=IMG_SHAPE,\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "\n",
    "    dense_base_model.trainable = False\n",
    "\n",
    "    dense_preprocess_input = tf.keras.applications.densenet.preprocess_input\n",
    "    dense_global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    dense_prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    dense_inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "    if data_aug:\n",
    "        dag = data_augmentation(dense_inputs)\n",
    "        dense_x =  tf.keras.applications.densenet.preprocess_input(dag)\n",
    "    else:\n",
    "        dense_x =  tf.keras.applications.densenet.preprocess_input(dense_inputs)\n",
    "    dense_x = dense_base_model(dense_x, training=False)\n",
    "    dense_x = dense_global_average_layer(dense_x)\n",
    "    if drop_value > 0:\n",
    "        dense_x = keras.layers.Dropout(drop_value)(dense_x)\n",
    "    dense_outputs = dense_prediction_layer(dense_x)\n",
    "    dense_model = tf.keras.Model(dense_inputs, dense_outputs)\n",
    "    return dense_base_model, dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_EPOCHS=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dense_model, denseNetModel = get_dense_net(0.0, True, 0.0)\n",
    "do_transfer_learning(0.0, True, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "def get_class_name(subdir):\n",
    "    return subdir[subdir[:subdir.rfind(\"/\")].rfind(\"/\")+1:subdir.rfind('/')]\n",
    "\n",
    "def get_file_name(subdir):\n",
    "    return subdir[subdir.rfind('/')+1:]\n",
    "\n",
    "def copy_results(files, tenancy, result):\n",
    "    for file in files:\n",
    "        class_name = get_class_name(file)\n",
    "        file_name = get_file_name(file)\n",
    "          \n",
    "        if not os.path.exists(os.path.join(os.getcwd(), final_dir)):\n",
    "            os.makedirs(os.path.join(os.getcwd(), final_dir), exist_ok=True)\n",
    "        copyfile(file, final_dir + \"/\"+file_name)\n",
    "    \n",
    "def process_results(correct_files, incorrect_files, tenancy):\n",
    "    copy_results(correct_files, tenancy, \"correct\")\n",
    "    copy_results(incorrect_files, tenancy, \"incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Resnet V2 Fine Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.inception_resnet_v2', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks = [mcp_save, early_stopping]\n",
    "\n",
    "def do_transfer_learning(drop_value=0.0, data_aug=False, l2_reg=0.0):\n",
    "    inception_resnet_base_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "    inception_resnet_base_model.trainable = False\n",
    "\n",
    "    ir_preprocess_input = tf.keras.applications.inception_resnet_v2.preprocess_input\n",
    "    ir_global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    ir_prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    ir_inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "    if data_aug:\n",
    "        dag = data_augmentation(ir_inputs)\n",
    "        ir_x =  tf.keras.applications.inception_resnet_v2.preprocess_input(dag)\n",
    "    else:\n",
    "        ir_x =  tf.keras.applications.inception_resnet_v2.preprocess_input(ir_inputs)\n",
    "    ir_x = inception_resnet_base_model(ir_x, training=False)\n",
    "    ir_x = ir_global_average_layer(ir_x)\n",
    "    if drop_value > 0:\n",
    "        ir_x = keras.layers.Dropout(drop_value)(ir_x)\n",
    "    ir_outputs = ir_prediction_layer(ir_x)\n",
    "    ir_model = tf.keras.Model(ir_inputs, ir_outputs)\n",
    "    \n",
    "    ir_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    ir_history = ir_model.fit(train_dataset,\n",
    "                    epochs=TRAINING_EPOCHS,\n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=callbacks)\n",
    "    \n",
    "    acc = ir_history.history['accuracy']\n",
    "    val_acc = ir_history.history['val_accuracy']\n",
    "\n",
    "    loss = ir_history.history['loss']\n",
    "    val_loss = ir_history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title(\"Training and Validation Accuracy with Dropout {0}\".format(drop_value))\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,1.0])\n",
    "    plt.title(\"Training and Validation Loss with Dropout {0}\".format(drop_value))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    inception_resnet_base_model.trainable = True\n",
    "    \n",
    "    ir_fine_tune_at = 755\n",
    "    \n",
    "    for layer in inception_resnet_base_model.layers[:ir_fine_tune_at]:\n",
    "        layer.trainable =  False\n",
    "        \n",
    "    ir_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    total_epochs =  TRAINING_EPOCHS + TRAINING_EPOCHS\n",
    "\n",
    "    ir_history_fine = ir_model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=ir_history.epoch[-1],\n",
    "                         validation_data=validation_dataset,\n",
    "                         callbacks=callbacks)\n",
    "    \n",
    "    acc += ir_history_fine.history['accuracy']\n",
    "    val_acc += ir_history_fine.history['val_accuracy']\n",
    "\n",
    "    loss += ir_history_fine.history['loss']\n",
    "    val_loss += ir_history_fine.history['val_loss']\n",
    "    \n",
    "    show_plot(acc, val_acc, loss, val_loss)\n",
    "    \n",
    "    loss, accuracy = ir_model.evaluate(test_dataset)\n",
    "    print('Test accuracy :', accuracy)\n",
    "    \n",
    "    print_tsne(ir_model, test_dataset, 2500)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_transfer_learning(0.0, True, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
