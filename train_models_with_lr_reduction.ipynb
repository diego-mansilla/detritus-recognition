{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-26 07:30:41.535179: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-12-26 07:30:50.357840: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-12-26 07:30:50.405348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-12-26 07:30:50.405881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:81:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-12-26 07:30:50.405937: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-12-26 07:30:50.599921: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-12-26 07:30:50.600035: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-12-26 07:30:50.672058: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-12-26 07:30:50.784306: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-12-26 07:30:50.809339: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-12-26 07:30:50.828152: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-12-26 07:30:50.844613: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-12-26 07:30:50.846527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2022-12-26 07:30:50.847268: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-26 07:30:51.333905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-12-26 07:30:51.334189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:81:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2022-12-26 07:30:51.335097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2022-12-26 07:30:51.335184: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-12-26 07:30:52.142646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-12-26 07:30:52.142693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2022-12-26 07:30:52.142702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N \n",
      "2022-12-26 07:30:52.142707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N \n",
      "2022-12-26 07:30:52.143865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\n",
      "2022-12-26 07:30:52.144451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10160 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1)\n",
      "2022-12-26 07:30:52.151480: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 8.56M (8978432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-12-26 07:30:52.152357: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 7.71M (8080640 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-12-26 07:30:52.153300: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 6.94M (7272704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-12-26 07:30:52.154182: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 6.24M (6545664 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from lib.methods import *\n",
    "from lib.models import *\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading SIPI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIPI dataset was preprocessed using prepare_dataset notebook, so these three folders already contain Detritus/Non-Detritus images.\n",
    "\n",
    "Training: 70%\n",
    "Validation: 15%\n",
    "Testing: 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetName = '../../Detritus/Dataset-Detritus-Bubble'\n",
    "\n",
    "train_dir = DatasetName+'/train'\n",
    "validation_dir =  DatasetName+'/val'\n",
    "test_dir = DatasetName+'/test'\n",
    "\n",
    "test_all_class_dir = '../../Detritus/Dataset-Detritus-Bubble/test'\n",
    "train_all_class_dir = 'Dataset-Detritus-Bubble/train'\n",
    "val_all_class_dir = 'Dataset-Detritus-Bubble/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three datasets are loaded using keras preprocessing method *image_dataset_from_directory*. Both the batch size and the image size hyperparameters where tested using different values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38391 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(train_dir,\n",
    "shuffle=True,\n",
    "batch_size=BATCH_SIZE,\n",
    "image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8226 files belonging to 2 classes.\n",
      "Found 8227 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = image_dataset_from_directory(validation_dir,\n",
    "shuffle=True,\n",
    "batch_size=BATCH_SIZE,\n",
    "image_size=IMG_SIZE)\n",
    "\n",
    "test_dataset = image_dataset_from_directory(test_dir,\n",
    "shuffle=True,\n",
    "batch_size=BATCH_SIZE,\n",
    "image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8227 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = ImageDataGenerator()\n",
    "test_data_generator = test_generator.flow_from_directory(test_all_class_dir,\n",
    "shuffle=False,\n",
    "batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamers to be used in all models\n",
    "base_learning_rate = 0.001\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "TRAINING_EPOCHS = 200\n",
    "\n",
    "\n",
    "# Callback Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                patience=20,\n",
    "                                                mode=\"min\",\n",
    "                                               restore_best_weights=True)\n",
    "\n",
    "reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, min_lr=1e-7, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-26 07:32:22.478605: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 2.94M (3087104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2022-12-26 07:32:22.479674: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 2.94M (3087104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.model_a_lr', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks = [mcp_save, early_stopping, reduce_lr_loss]\n",
    "\n",
    "model_A = get_model_A(0.0, True, 0.0)\n",
    "history = train_model(model_A, TRAINING_EPOCHS, train_dataset, validation_dataset, base_learning_rate, callbacks)\n",
    "show_plot(history, 0.0)\n",
    "test_accuracy(model_A, test_dataset)\n",
    "print_tsne(model_A, test_dataset, 2500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.model_b_lr', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks = [mcp_save, early_stopping, reduce_lr_loss]\n",
    "\n",
    "model_B = get_model_B(0.0, True, 0.0)\n",
    "history = train_model(model_B, TRAINING_EPOCHS, train_dataset, validation_dataset, base_learning_rate, callbacks)\n",
    "show_plot(history, 0.0)\n",
    "test_accuracy(model_B, test_dataset)\n",
    "print_tsne(model_B, test_dataset, 2500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mobile_net(drop_value=0.0, data_aug=False, l2_reg=0.0):\n",
    "    inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "    \n",
    "    if data_aug:\n",
    "        dag = data_augmentation(inputs)\n",
    "        processed_input =  tf.keras.applications.mobilenet_v2.preprocess_input(dag)\n",
    "    else:\n",
    "        processed_input =  tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "    \n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights=None)\n",
    "    \n",
    "    x = base_model(processed_input)\n",
    "    \n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    \n",
    "    x = global_average_layer(x)\n",
    "    \n",
    "    if drop_value > 0:\n",
    "        x = keras.layers.Dropout(drop_value)(x)\n",
    "    \n",
    "    if l2_reg > 0:\n",
    "        x = keras.layers.Dense(1, kernel_regularizer=regularizers.l2(l2_reg),\n",
    "                bias_regularizer=regularizers.l2(l2_reg),\n",
    "                activity_regularizer=regularizers.l2(l2_reg))(x)\n",
    "    else:\n",
    "        x = keras.layers.Dense(1)(x)\n",
    "    return keras.Model(inputs, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNetV2 Model From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.mobile_net_scratch_lr', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks = [mcp_save, early_stopping, reduce_lr_loss]\n",
    "\n",
    "mobileNetModelSc = get_mobile_net(0.0, True, 0.0)\n",
    "history = train_model(mobileNetModelSc, TRAINING_EPOCHS, train_dataset, validation_dataset, base_learning_rate, callbacks)\n",
    "show_plot(history, 0.0)\n",
    "test_accuracy(mobileNetModelSc, test_dataset)\n",
    "print_tsne(mobileNetModelSc, test_dataset, 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.dense_net_ft_lr', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks = [mcp_save, early_stopping, reduce_lr_loss]\n",
    "\n",
    "def do_transfer_learning(drop_value=0.0, data_aug=False, l2_reg=0.0, input_model=None, base_model=None):\n",
    "    dense_model = input_model\n",
    "    dense_base_model = base_model\n",
    "    if (input_model is None):\n",
    "        dense_base_model = tf.keras.applications.DenseNet201(input_shape=IMG_SHAPE,\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "\n",
    "        dense_base_model.trainable = False\n",
    "\n",
    "        dense_preprocess_input = tf.keras.applications.densenet.preprocess_input\n",
    "        dense_global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        dense_prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "        dense_inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "        if data_aug:\n",
    "            dag = data_augmentation(dense_inputs)\n",
    "            dense_x =  tf.keras.applications.densenet.preprocess_input(dag)\n",
    "        else:\n",
    "            dense_x =  tf.keras.applications.densenet.preprocess_input(dense_inputs)\n",
    "        dense_x = dense_base_model(dense_x, training=False)\n",
    "        dense_x = dense_global_average_layer(dense_x)\n",
    "        if drop_value > 0:\n",
    "            dense_x = keras.layers.Dropout(drop_value)(dense_x)\n",
    "        dense_outputs = dense_prediction_layer(dense_x)\n",
    "        dense_model = tf.keras.Model(dense_inputs, dense_outputs)\n",
    "\n",
    "    dense_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    dense_history = dense_model.fit(train_dataset,\n",
    "                    epochs=TRAINING_EPOCHS,\n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=callbacks)\n",
    "    \n",
    "    acc = dense_history.history['accuracy']\n",
    "    val_acc = dense_history.history['val_accuracy']\n",
    "\n",
    "    loss = dense_history.history['loss']\n",
    "    val_loss = dense_history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title(\"Training and Validation Accuracy with Dropout {0}\".format(drop_value))\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,1.0])\n",
    "    plt.title(\"Training and Validation Loss with Dropout {0}\".format(drop_value))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    dense_base_model.trainable = True\n",
    "    \n",
    "    dense_fine_tune_at = 660\n",
    "    \n",
    "    for layer in dense_base_model.layers[:dense_fine_tune_at]:\n",
    "        layer.trainable =  False\n",
    "        \n",
    "    dense_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    total_epochs =  TRAINING_EPOCHS + TRAINING_EPOCHS\n",
    "\n",
    "    dense_history_fine = dense_model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=dense_history.epoch[-1],\n",
    "                         validation_data=validation_dataset,\n",
    "                         callbacks=callbacks)\n",
    "    \n",
    "    acc += dense_history_fine.history['accuracy']\n",
    "    val_acc += dense_history_fine.history['val_accuracy']\n",
    "\n",
    "    loss += dense_history_fine.history['loss']\n",
    "    val_loss += dense_history_fine.history['val_loss']\n",
    "    \n",
    "    loss, accuracy = dense_model.evaluate(test_dataset)\n",
    "    print('Test accuracy :', accuracy)\n",
    "    \n",
    "    \n",
    "    show_plot(dense_history_fine, 0.0)\n",
    "    print_tsne(dense_model, test_dataset, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_net(drop_value=0.0, data_aug=False, l2_reg=0.0):\n",
    "    dense_base_model = tf.keras.applications.DenseNet201(input_shape=IMG_SHAPE,\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "\n",
    "    dense_base_model.trainable = False\n",
    "\n",
    "    dense_preprocess_input = tf.keras.applications.densenet.preprocess_input\n",
    "    dense_global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    dense_prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    dense_inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "    if data_aug:\n",
    "        dag = data_augmentation(dense_inputs)\n",
    "        dense_x =  tf.keras.applications.densenet.preprocess_input(dag)\n",
    "    else:\n",
    "        dense_x =  tf.keras.applications.densenet.preprocess_input(dense_inputs)\n",
    "    dense_x = dense_base_model(dense_x, training=False)\n",
    "    dense_x = dense_global_average_layer(dense_x)\n",
    "    if drop_value > 0:\n",
    "        dense_x = keras.layers.Dropout(drop_value)(dense_x)\n",
    "    dense_outputs = dense_prediction_layer(dense_x)\n",
    "    dense_model = tf.keras.Model(dense_inputs, dense_outputs)\n",
    "    return dense_base_model, dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dense_model, denseNetModel = get_dense_net(0.0, True, 0.0)\n",
    "do_transfer_learning(0.0, True, 0.0, denseNetModel, base_dense_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "def get_class_name(subdir):\n",
    "    return subdir[subdir[:subdir.rfind(\"/\")].rfind(\"/\")+1:subdir.rfind('/')]\n",
    "\n",
    "def get_file_name(subdir):\n",
    "    return subdir[subdir.rfind('/')+1:]\n",
    "\n",
    "def copy_results(files, tenancy, result):\n",
    "    for file in files:\n",
    "        class_name = get_class_name(file)\n",
    "        file_name = get_file_name(file)\n",
    "          \n",
    "        if not os.path.exists(os.path.join(os.getcwd(), final_dir)):\n",
    "            os.makedirs(os.path.join(os.getcwd(), final_dir), exist_ok=True)\n",
    "        copyfile(file, final_dir + \"/\"+file_name)\n",
    "    \n",
    "def process_results(correct_files, incorrect_files, tenancy):\n",
    "    copy_results(correct_files, tenancy, \"correct\")\n",
    "    copy_results(incorrect_files, tenancy, \"incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception Resnet V2 Fine Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('.inception_resnet_v2_lr', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks = [mcp_save, early_stopping, reduce_lr_loss]\n",
    "\n",
    "def do_transfer_learning(drop_value=0.0, data_aug=False, l2_reg=0.0):\n",
    "    inception_resnet_base_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "    inception_resnet_base_model.trainable = False\n",
    "\n",
    "    ir_preprocess_input = tf.keras.applications.inception_resnet_v2.preprocess_input\n",
    "    ir_global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    ir_prediction_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    ir_inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "    if data_aug:\n",
    "        dag = data_augmentation(ir_inputs)\n",
    "        ir_x =  tf.keras.applications.inception_resnet_v2.preprocess_input(dag)\n",
    "    else:\n",
    "        ir_x =  tf.keras.applications.inception_resnet_v2.preprocess_input(ir_inputs)\n",
    "    ir_x = inception_resnet_base_model(ir_x, training=False)\n",
    "    ir_x = ir_global_average_layer(ir_x)\n",
    "    if drop_value > 0:\n",
    "        ir_x = keras.layers.Dropout(drop_value)(ir_x)\n",
    "    ir_outputs = ir_prediction_layer(ir_x)\n",
    "    ir_model = tf.keras.Model(ir_inputs, ir_outputs)\n",
    "    \n",
    "    ir_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    ir_history = ir_model.fit(train_dataset,\n",
    "                    epochs=TRAINING_EPOCHS,\n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=callbacks)\n",
    "    \n",
    "    acc = ir_history.history['accuracy']\n",
    "    val_acc = ir_history.history['val_accuracy']\n",
    "\n",
    "    loss = ir_history.history['loss']\n",
    "    val_loss = ir_history.history['val_loss']\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title(\"Training and Validation Accuracy with Dropout {0}\".format(drop_value))\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,1.0])\n",
    "    plt.title(\"Training and Validation Loss with Dropout {0}\".format(drop_value))\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    inception_resnet_base_model.trainable = True\n",
    "    \n",
    "    ir_fine_tune_at = 755\n",
    "    \n",
    "    for layer in inception_resnet_base_model.layers[:ir_fine_tune_at]:\n",
    "        layer.trainable =  False\n",
    "        \n",
    "    ir_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    total_epochs =  TRAINING_EPOCHS + TRAINING_EPOCHS\n",
    "\n",
    "    ir_history_fine = ir_model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=ir_history.epoch[-1],\n",
    "                         validation_data=validation_dataset,\n",
    "                         callbacks=[callback])\n",
    "    \n",
    "    acc += ir_history_fine.history['accuracy']\n",
    "    val_acc += ir_history_fine.history['val_accuracy']\n",
    "\n",
    "    loss += ir_history_fine.history['loss']\n",
    "    val_loss += ir_history_fine.history['val_loss']\n",
    "    \n",
    "    loss, accuracy = ir_model.evaluate(test_dataset)\n",
    "    print('Test accuracy :', accuracy)\n",
    "    \n",
    "    show_plot(ir_history_fine, 0.0)\n",
    "    print_tsne(ir_model, test_dataset, 2500)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_transfer_learning(0.0, True, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
